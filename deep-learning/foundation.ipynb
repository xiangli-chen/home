{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative \n",
    "**Derivative** of univariate function: \n",
    "$$\n",
    "\\frac{df(x)}{dx} = \\lim_{\\Delta x\\ \\to 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}.\n",
    "$$\n",
    "The derivate tells us how sensitive of $f(x)$ on $x$.  \n",
    "**Partial Derivative** of multivariate function:\n",
    "$$\n",
    "\\frac{\\partial f(x,y)}{\\partial x} = \\lim_{\\Delta x\\ \\to 0} \\frac{f(x + \\Delta x, y) - f(x, y)}{\\Delta x}.\n",
    "$$\n",
    "The partial derivate with respect to $x$ tells us how sensitive of $f(x,y)$ on $x$ given a value of $y$.  \n",
    "**Chain Rule**   \n",
    "Univeriate case  \n",
    "Suppose $u=\\phi(x)$ and $v=\\psi(x)$, under some conditions, then\n",
    "$$\n",
    "\\frac{df(\\phi(x),\\psi(x))}{dx}=\n",
    "\\frac{\\partial f(u,v)}{\\partial u}\\frac{d\\phi(x)}{dx}+\n",
    "\\frac{\\partial f(u,v)}{\\partial v}\\frac{d\\psi(x)}{dx}.\n",
    "$$\n",
    "Multivariate case  \n",
    "Suppose $u=\\phi(x,y)$ and $v=\\psi(x,y)$, under some conditions, then\n",
    "$$\n",
    "\\frac{\\partial f(\\phi(x,y),\\psi(x,y))}{\\partial x}=\n",
    "\\frac{\\partial f(u,v)}{\\partial u}\\frac{\\partial \\phi(x,y)}{\\partial x}+\n",
    "\\frac{\\partial f(u,v)}{\\partial v}\\frac{\\partial \\psi(x,y)}{\\partial x}.\n",
    "$$\n",
    "A special case, suppose $u=\\phi(x,y)$, $v=\\psi(x)$ and $w=\\omega(y)$, under some conditions, then\n",
    "$$\n",
    "\\frac{\\partial f(\\phi(x,y),\\psi(x),\\omega(y),x,y)}{\\partial x}=\n",
    "\\frac{\\partial f(u,v,w,x,y)}{\\partial u}\\frac{\\partial \\phi(x,y)}{\\partial x}+\n",
    "\\frac{\\partial f(u,v,w,x,y)}{\\partial v}\\frac{d\\psi(x)}{dx}+\n",
    "\\frac{\\partial f(u,v,w,x,y)}{\\partial x}.\n",
    "$$\n",
    "Note that though $f(\\phi(x,y),\\psi(x),\\omega(y),x,y)=f(u,v,w,x,y)$, but their partial derivative with respect to $x$ are different.\n",
    "$\\partial f(\\phi(x,y),\\psi(x),\\omega(y),x,y)/\\partial x$ consider $y$ is a constant with respect to $x$, while $\\partial f(u,v,w,x,y)/\\partial x$ consider $u,v,w$ and $y$ are constants with respect to $x$.   \n",
    "Suppose $\\theta = (\\theta^1,\\cdots,\\theta^n)$, we may apply chain rule recursively to get the partial derivative $\\partial f(\\theta)/\\partial \\theta_i$ for $\\forall i\\in \\{1,\\cdots,n\\}$.  \n",
    "Belw is an example of using chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprobagation\n",
    "### A simple example\n",
    "Suppose $q=\\phi(x,y)=x+y$ and $f(\\phi(x,y),z)=\\phi(x,y)*z=(x+y)*z$, so $f(\\phi(x,y),z)=f(q,z)=q*z$. The partial derivatives with respect to $x, y$ and $z$ are\n",
    "$$\n",
    "\\frac{\\partial f(\\phi(x,y),z)}{\\partial x} = \\frac{\\partial f(q,z)}{\\partial q}\n",
    "\\frac{\\partial \\phi(x,y)}{\\partial x} = z\\times 1 = z,\\\\\n",
    "\\frac{\\partial f(\\phi(x,y),z)}{\\partial y} = \\frac{\\partial f(q,z)}{\\partial q}\n",
    "\\frac{\\partial \\phi(x,y)}{\\partial y} = z\\times 1 = z,\\\\\n",
    "\\frac{\\partial f(\\phi(x,y),z)}{\\partial z} = \\frac{\\partial f(q,z)}{\\partial z}=q.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.0, -4.0, 3]\n"
     ]
    }
   ],
   "source": [
    "# set some inputs\n",
    "x = -2; y = 5; z = -4\n",
    "\n",
    "# perform the forward pass\n",
    "q = x + y # q becomes 3\n",
    "f = q * z # f becomes -12\n",
    "\n",
    "# perform the backward pass (backpropagation) in reverse order:\n",
    "# first backprop through f = q * z\n",
    "dfdz = q # df/dz = q, so gradient on z becomes 3\n",
    "dfdq = z # df/dq = z, so gradient on q becomes -4\n",
    "# now backprop through q = x + y\n",
    "dfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!\n",
    "dfdy = 1.0 * dfdq # dq/dy = 1\n",
    "print([dfdx, dfdy, dfdz])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation above can be visualized with a circuit diagram:\n",
    "![circuit-simple](./figures/fd/circuit-simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition of backprobagation**  \n",
    "After  performing the forward pass, we compute the partial derivative with respect to $q$ and use this result to compute the partial derivative with respect to $x$ and $y$ using chain rult. Backprobagation is a partial derivative computation procedure that starts at the output layer until the input layer working in backward order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition of parallel computing**  \n",
    "After performing the forward pass, computing the partial derivatives with respect to $q$ and $z$ are independent which can be computed in parallel way. Knowing the partial derivative with respect to $q$, computing the partial derivatives with respect to $x$ and $y$ are independent which also can be computed in parallel way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Example  \n",
    "Suppose we have the sigmoid function below:\n",
    "$$\n",
    "f_x(w) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}},\n",
    "$$\n",
    "where $x$ is the input and $w$ is the parameter for which we want to compute the partial derivative with respect to each of its elements.  \n",
    "Let \n",
    "$$\n",
    "u = w_0x_0 + w_1x_1 + w_2,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "g_x(u) = \\frac{1}{1+e^{-u}} = f_x(w).\n",
    "$$\n",
    "Using chain rule, the partial derivative with respect to $w_0$ is\n",
    "$$\n",
    "\\frac{\\partial f_x(w)}{\\partial w_0}=\\frac{\\partial g_x(u(w))}{\\partial w_0}=\n",
    "\\frac{g_x(u)}{\\partial u}\\frac{\\partial u(w)}{\\partial w_0}.\n",
    "$$\n",
    "The partial derivative of $g_x(u)$ with respect to $u$ is\n",
    "$$\n",
    "\\frac{\\partial g_x(u)}{\\partial u} = (1-g_x(u))g_x(u)\n",
    "$$\n",
    "and the partial derivative of $u(w)$ with respect to $w_0$ is straightforward which is $x_0$.\n",
    "Similarly, we can easily derive the partial derivative with respect to $w_1$ and $w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.19661193324148185, -0.3932238664829637, 0.19661193324148185]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "w = [2,-3,-3] # assume some random weights and data\n",
    "x = [-1, -2]\n",
    "\n",
    "# forward pass\n",
    "u = w[0]*x[0] + w[1]*x[1] + w[2]\n",
    "g = 1.0 / (1 + math.exp(-u)) # sigmoid function\n",
    "\n",
    "# backward pass through the neuron (backpropagation)\n",
    "dgdu = (1 - g) * g # gradient on dot variable, using the sigmoid gradient derivation\n",
    "dgdw = [x[0] * dgdu, x[1] * dgdu, 1.0 * dgdu] # backprop into w\n",
    "# we're done! we have the partial derivatives on the inputs to the circuit\n",
    "print(dgdw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staged Computation  \n",
    "To be continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Operation  \n",
    "To be continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "Let $e_l=(\\cos\\alpha,\\cos\\beta)$ be a **unit vector** of the direction $l$ on a 2-D plane $(x,y)$, then the **directional derivative** of $f(x,y)$ with respect to $l$ is\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial l} = \\lim_{t\\ \\to 0^+} \\frac{f(x + t\\cos\\alpha, y+t\\cos\\beta) - f(x, y)}{t}.\n",
    "$$\n",
    "Directional derivative tells us how sensitive of $f(x,y)$ on the direction $l$ defined by $e_l=(\\cos\\alpha,\\cos\\beta)$.  \n",
    "A theorem shows that\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial l} = \\frac{\\partial f(x,y)}{\\partial x}\\cos\\alpha+\n",
    "\\frac{\\partial f(x,y)}{\\partial y}\\cos\\beta\n",
    "$$\n",
    "The **gradient** \\\\(\\nabla f\\\\) of $f(x,y)$ is the vector of partial derivatives that \n",
    "$$\n",
    "\\nabla f = (\\frac{\\partial f(x,y)}{\\partial x}, \\frac{\\partial f(x,y)}{\\partial y}),\n",
    "$$ \n",
    "and then\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial l}=\\nabla f\\cdot e_l= |\\nabla||e_l|\\cos\\theta=|\\nabla|\\cos\\theta\n",
    "$$\n",
    "where $\\theta$ is the angle between $\\nabla f$ and $e_l$.   \n",
    "So when $\\theta = 0$ that $\\nabla f$ and $e_l$ have the same direction, the partial derivative are maximized which motivates the **gradient learning method**.\n",
    "Gradient is the most efficient direction for converging to an optimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
