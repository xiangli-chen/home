{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative \n",
    "**Derivative** of univariate function: \n",
    "$$\n",
    "\\frac{df(x)}{dx} = \\lim_{\\Delta x\\ \\to 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}.\n",
    "$$\n",
    "The derivate tells us how sensitive of $f(x)$ on $x$.  \n",
    "**Partial Derivative** of multivariate function:\n",
    "$$\n",
    "\\frac{\\partial f(x,y)}{\\partial x} = \\lim_{\\Delta x\\ \\to 0} \\frac{f(x + \\Delta x, y) - f(x, y)}{\\Delta x}.\n",
    "$$\n",
    "The partial derivate with respect to $x$ tells us how sensitive of $f(x,y)$ on $x$ given a value of $y$.  \n",
    "**Chain Rule**   \n",
    "Univeriate case  \n",
    "Suppose $u=\\phi(x)$ and $v=\\psi(x)$, under some conditions, then\n",
    "$$\n",
    "\\frac{df(\\phi(x),\\psi(x))}{dx}=\n",
    "\\frac{\\partial f(u,v)}{\\partial u}\\frac{d\\phi(x)}{dx}+\n",
    "\\frac{\\partial f(u,v)}{\\partial v}\\frac{d\\psi(x)}{dx}.\n",
    "$$\n",
    "Multivariate case  \n",
    "Suppose $u=\\phi(x,y)$ and $v=\\psi(x,y)$, under some conditions, then\n",
    "$$\n",
    "\\frac{\\partial f(\\phi(x,y),\\psi(x,y))}{\\partial x}=\n",
    "\\frac{\\partial f(u,v)}{\\partial u}\\frac{\\partial \\phi(x,y)}{\\partial x}+\n",
    "\\frac{\\partial f(u,v)}{\\partial v}\\frac{\\partial \\psi(x,y)}{\\partial x}.\n",
    "$$\n",
    "A special case, suppose $u=\\phi(x,y)$, $v=\\psi(x)$ and $w=\\omega(y)$, under some conditions, then\n",
    "$$\n",
    "\\frac{\\partial f(\\phi(x,y),\\psi(x),\\omega(y),x,y)}{\\partial x}=\n",
    "\\frac{\\partial f(u,v,w,x,y)}{\\partial u}\\frac{\\partial \\phi(x,y)}{\\partial x}+\n",
    "\\frac{\\partial f(u,v,w,x,y)}{\\partial v}\\frac{d\\psi(x)}{dx}+\n",
    "\\frac{\\partial f(u,v,w,x,y)}{\\partial x}.\n",
    "$$\n",
    "Note that though $f(\\phi(x,y),\\psi(x),\\omega(y),x,y)=f(u,v,w,x,y)$, but their partial derivative with respect to $x$ are different.\n",
    "$\\partial f(\\phi(x,y),\\psi(x),\\omega(y),x,y)/\\partial x$ consider $y$ is a constant with respect to $x$, while $\\partial f(u,v,w,x,y)/\\partial x$ consider $u,v,w$ and $y$ are constants with respect to $x$.   \n",
    "Suppose $\\theta = (\\theta^1,\\cdots,\\theta^n)$, we may apply chain rule recursively to get the partial derivative $\\partial f(\\theta)/\\partial \\theta_i$ for $\\forall i\\in \\{1,\\cdots,n\\}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "Let $e_l=(\\cos\\alpha,\\cos\\beta)$ be a **unit vector** of the direction $l$ on a 2-D plane $(x,y)$, then the **directional derivative** of $f(x,y)$ with respect to $l$ is\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial l} = \\lim_{t\\ \\to 0^+} \\frac{f(x + t\\cos\\alpha, y+t\\cos\\beta) - f(x, y)}{t}.\n",
    "$$\n",
    "Directional derivative tells us how sensitive of $f(x,y)$ on the direction $l$ defined by $e_l=(\\cos\\alpha,\\cos\\beta)$.  \n",
    "A theorem shows that\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial l} = \\frac{\\partial f(x,y)}{\\partial x}\\cos\\alpha+\n",
    "\\frac{\\partial f(x,y)}{\\partial y}\\cos\\beta\n",
    "$$\n",
    "The **gradient** \\\\(\\nabla f\\\\) of $f(x,y)$ is the vector of partial derivatives that \n",
    "$$\n",
    "\\nabla f = (\\frac{\\partial f(x,y)}{\\partial x}, \\frac{\\partial f(x,y)}{\\partial y}),\n",
    "$$ \n",
    "and then\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial l}=\\nabla f\\cdot e_l= |\\nabla||e_l|\\cos\\theta=|\\nabla|\\cos\\theta\n",
    "$$\n",
    "where $\\theta$ is the angle between $\\nabla f$ and $e_l$.   \n",
    "So when $\\theta = 0$ that $\\nabla f$ and $e_l$ have the same direction, the partial derivative are maximized which motivates the **gradient learning method**.\n",
    "Gradient is the most efficient direction for converging to an optimal point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprobagation\n",
    "### A simple example\n",
    "Suppose $q=\\phi(x,y)=x+y$ and $f(\\phi(x,y),z)=\\phi(x,y)*z=(x+y)*z$, so $f(\\phi(x,y),z)=f(q,z)=q*z$. The partial derivatives with respect to $x, y$ and $z$ are\n",
    "$$\n",
    "\\frac{\\partial f(\\phi(x,y),z)}{\\partial x} = \\frac{\\partial f(q,z)}{\\partial q}\n",
    "\\frac{\\partial \\phi(x,y)}{\\partial x} = z\\times 1 = z,\\\\\n",
    "\\frac{\\partial f(\\phi(x,y),z)}{\\partial y} = \\frac{\\partial f(q,z)}{\\partial q}\n",
    "\\frac{\\partial \\phi(x,y)}{\\partial y} = z\\times 1 = z,\\\\\n",
    "\\frac{\\partial f(\\phi(x,y),z)}{\\partial z} = \\frac{\\partial f(q,z)}{\\partial z}=q.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.0, -4.0, 3]\n"
     ]
    }
   ],
   "source": [
    "# set some inputs\n",
    "x = -2; y = 5; z = -4\n",
    "\n",
    "# perform the forward pass\n",
    "q = x + y # q becomes 3\n",
    "f = q * z # f becomes -12\n",
    "\n",
    "# perform the backward pass (backpropagation) in reverse order:\n",
    "# first backprop through f = q * z\n",
    "dfdz = q # df/dz = q, so gradient on z becomes 3\n",
    "dfdq = z # df/dq = z, so gradient on q becomes -4\n",
    "# now backprop through q = x + y\n",
    "dfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!\n",
    "dfdy = 1.0 * dfdq # dq/dy = 1\n",
    "print([dfdx, dfdy, dfdz])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation above can be visualized with a circuit diagram:\n",
    "![circuit-simple](./figures/fd/circuit-simple.png)\n",
    "**Intuition of backprobagation**  \n",
    "After  performing the forward pass, we compute the partial derivative with respect to $q$ and use this result to compute the partial derivative with respect to $x$ and $y$ using chain rult. Backprobagation is a partial derivative computation procedure that starts at the output layer until the input layer working in backward order.  \n",
    "**Intuition of parallel computing**  \n",
    "After performing the forward pass, computing the partial derivatives with respect to $q$ and $z$ are independent which can be computed in parallel way. Knowing the partial derivative with respect to $q$, computing the partial derivatives with respect to $x$ and $y$ are independent which also can be computed in parallel way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid example  \n",
    "Suppose we have the sigmoid function below:\n",
    "$$\n",
    "f_x(w) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}},\n",
    "$$\n",
    "where $x$ is the input and $w$ is the parameter for which we want to compute the partial derivative with respect to each of its elements.  \n",
    "Let \n",
    "$$\n",
    "u = w_0x_0 + w_1x_1 + w_2,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "g_x(u) = \\frac{1}{1+e^{-u}} = f_x(w).\n",
    "$$\n",
    "Using chain rule, the partial derivative with respect to $w_0$ is\n",
    "$$\n",
    "\\frac{\\partial f_x(w)}{\\partial w_0}=\\frac{\\partial g_x(u(w))}{\\partial w_0}=\n",
    "\\frac{g_x(u)}{\\partial u}\\frac{\\partial u(w)}{\\partial w_0}.\n",
    "$$\n",
    "The partial derivative of $g_x(u)$ with respect to $u$ is\n",
    "$$\n",
    "\\frac{\\partial g_x(u)}{\\partial u} = (1-g_x(u))g_x(u)\n",
    "$$\n",
    "and the partial derivative of $u(w)$ with respect to $w_0$ is straightforward which is $x_0$.\n",
    "Similarly, we can easily derive the partial derivative with respect to $w_1$ and $w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.19661193324148185, -0.3932238664829637, 0.19661193324148185]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "w = [2,-3,-3] # assume some random weights and data\n",
    "x = [-1, -2]\n",
    "\n",
    "# forward pass\n",
    "u = w[0]*x[0] + w[1]*x[1] + w[2]\n",
    "g = 1.0 / (1 + math.exp(-u)) # sigmoid function\n",
    "\n",
    "# backward pass through the neuron (backpropagation)\n",
    "dgdu = (1 - g) * g # gradient on dot variable, using the sigmoid gradient derivation\n",
    "dgdw = [x[0] * dgdu, x[1] * dgdu, 1.0 * dgdu] # backprop into w\n",
    "# we're done! we have the partial derivatives on the inputs to the circuit\n",
    "print(dgdw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staged computation  \n",
    "Let\n",
    "$$\n",
    "f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2},\n",
    "$$\n",
    "where $\\sigma(x)=1/(1+e^{-x})$.  \n",
    "Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 3 # example values\n",
    "y = -4\n",
    "\n",
    "# forward pass\n",
    "sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)\n",
    "num = x + sigy # numerator                               #(2)\n",
    "sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)\n",
    "xpy = x + y                                              #(4)\n",
    "xpysqr = xpy**2                                          #(5)\n",
    "den = sigx + xpysqr # denominator                        #(6)\n",
    "invden = 1.0 / den                                       #(7)\n",
    "f = num * invden # done!                                 #(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backprobagation**  \n",
    "We'll go backwards and for every variable along the way in the forward pass (`sigy, num, sigx, xpy, xpysqr, den, invden`) we will have the same variable, but one that begins with a `d`, which will hold the gradient of the output of the circuit with respect to that variable.  \n",
    "Note that every single piece in backprop will involve computing the local gradient of that expression, and chaining it with the gradient on that expression with a multiplication.  \n",
    "**Cache forward pass variables**. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0595697955721652 1.5922327514838093\n"
     ]
    }
   ],
   "source": [
    "# backprop f = num * invden\n",
    "dnum = invden # gradient on numerator                             #(8)\n",
    "dinvden = num                                                     #(8)\n",
    "# backprop invden = 1.0 / den \n",
    "dden = (-1.0 / (den**2)) * dinvden                                #(7)\n",
    "# backprop den = sigx + xpysqr\n",
    "dsigx = (1) * dden                                                #(6)\n",
    "dxpysqr = (1) * dden                                              #(6)\n",
    "# backprop xpysqr = xpy**2\n",
    "dxpy = (2 * xpy) * dxpysqr                                        #(5)\n",
    "# backprop xpy = x + y\n",
    "dx = (1) * dxpy                                                   #(4)\n",
    "dy = (1) * dxpy                                                   #(4)\n",
    "# backprop sigx = 1.0 / (1 + math.exp(-x))\n",
    "dx += ((1 - sigx) * sigx) * dsigx # Notice += !!                  #(3)\n",
    "# backprop num = x + sigy\n",
    "dx += (1) * dnum                                                  #(2)\n",
    "dsigy = (1) * dnum                                                #(2)\n",
    "# backprop sigy = 1.0 / (1 + math.exp(-y))\n",
    "dy += ((1 - sigy) * sigy) * dsigy                                 #(1)\n",
    "# done! phew\n",
    "print(dx,dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns in backward flow  \n",
    "In many cases the backward-flowing gradient can be interpreted on an intuitive level.\n",
    "Gates such as add,mul,max all have very simple interpretations in terms of how they act during backpropagation.\n",
    "![backward-flow-pattern](./figures/fd/backward-flow-pattern.png)\n",
    "**Add gate**  \n",
    "Suppose $u=x+y$,\n",
    "$$\n",
    "\\frac{\\partial f(u(x))}{\\partial x} = \\frac{\\partial f(u)}{\\partial u}\\frac{\\partial u(x)}{\\partial x} = \\frac{\\partial f(u)}{\\partial u}.\n",
    "$$\n",
    "The add gate always takes the gradient on its output and distributes it equally to all of its inputs.  \n",
    "**Max gate**  \n",
    "Suppose $u=max(x,y)$\n",
    "$$\n",
    "\\frac{\\partial f(u(x))}{\\partial x} = \\frac{\\partial f(u)}{\\partial u}\\frac{\\partial u(x)}{\\partial x} = \\frac{\\partial f(u)}{\\partial u}1_{(x>=y)}.\n",
    "$$\n",
    "The max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass).  \n",
    "**Multiply gate**  \n",
    "Suppose $u=xy$\n",
    "$$\n",
    "\\frac{\\partial f(u(x))}{\\partial x} = \\frac{\\partial f(u)}{\\partial u}\\frac{\\partial u(x)}{\\partial x} = \\frac{\\partial f(u)}{\\partial u}y.\n",
    "$$\n",
    "As for the multiply gate, its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule.  \n",
    "Notice that if one of the inputs to the multiply gate is very small and the other is very big, it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. The scale of the data has an effect on the magnitude of the gradient for its weights. Therefore, preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help for debugging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Operation  \n",
    "**Matrix-Matrix multiply gradient**  \n",
    "A [notes](./literature/vecDerivs.pdf) by Erik Learned-Miller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# forward pass\n",
    "W = np.random.randn(5, 10)\n",
    "X = np.random.randn(10, 3)\n",
    "D = W.dot(X)\n",
    "\n",
    "# now suppose we had the gradient on D from above in the circuit\n",
    "dD = np.random.randn(*D.shape) # same shape as D\n",
    "dW = dD.dot(X.T) #.T gives the transpose of the matrix\n",
    "dX = W.T.dot(dD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "* Break up functions into modules for which we can easily derive local gradients, and then chain them with chain rule. \n",
    "* Never need an explicit mathematical equation for the gradient of the input variables. \n",
    "* Decompose expressions into stages such that every stage can be differentiated independently (the stages will be matrix vector multiplies, or max operations, or sum operations, etc.) and then backprop through the variables one step at a time.  \n",
    "\n",
    "## Reference\n",
    "* [Automatic differentiation in machine learning: a survey](http://arxiv.org/abs/1502.05767)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
