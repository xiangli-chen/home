{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "Table of Contents:\n",
    "\n",
    "- [Architecture Overview](#overview)\n",
    "- [ConvNet Layers](#layers)\n",
    "  - [Convolutional Layer](#conv)\n",
    "  - [Pooling Layer](#pool)\n",
    "  - [Normalization Layer](#norm)\n",
    "  - [Converting Fully-Connected Layers to Convolutional Layers](#convert)\n",
    "- [ConvNet Architectures](#architectures)\n",
    "  - [Layer Patterns](#layerpat)\n",
    "  - [Layer Sizing Patterns](#layersizepat)\n",
    "  - [Case Studies](#case) (LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet)\n",
    "  - [Computational Considerations](#comp)\n",
    "- [Additional References](#add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='overview'></a>\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Regular Neural Nets don't scale well to full images. **Convolutional Neural Networks** take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular: \n",
    "- The layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. \n",
    "- The neurons in a layer will only be connected to a small region of the layer before it. \n",
    "- By the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension. \n",
    "\n",
    "Here is a visualization:\n",
    "\n",
    "![convolutional-neural-networks](./figures/cnn/cnn.jpeg)\n",
    "\n",
    "*A ConvNet arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels).*\n",
    "\n",
    "> A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='layers'></a>\n",
    "\n",
    "## Layers used to build ConvNets \n",
    "\n",
    "There are three main types of layers to build ConvNet architectures: **Convolutional Layer**, **Pooling Layer**, and **Fully-Connected Layer**.\n",
    "\n",
    "A simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC].\n",
    "\n",
    "- **INPUT** [32x32x3] will hold the raw pixel values of the image. An image of width 32, height 32, and with three color channels R,G,B.\n",
    "- **CONV layer** will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.\n",
    "- **RELU layer** will apply an elementwise activation function, such as the \\\\(max(0,x)\\\\) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).\n",
    "- **POOL layer** will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].\n",
    "- **FC (i.e. fully-connected) layer** will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score. Each neuron in this layer will be connected to all the numbers in the previous volume.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)\n",
    "- There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)\n",
    "- Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function\n",
    "- Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don't)\n",
    "- Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn't)\n",
    "\n",
    "![convnet-car](./figures/cnn/convnet-car.jpeg)\n",
    "*The activations of an example ConvNet architecture. The initial volume stores the raw image pixels (left) and the last volume stores the class scores (right). Each volume of activations along the processing path is shown as a column. Since it's difficult to visualize 3D volumes, we lay out each volume's slices in rows. The last layer volume holds the scores for each class, but here we only visualize the sorted top 5 scores, and print the labels of each one. The architecture shown here is a tiny VGG Net.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "\n",
    "**Overview and intuition.** \n",
    "- The convolutional layer's parameters consist of a set of learnable filters. \n",
    "- Every filter is small spatially (along width and height), but extends through the full depth of the input volume. \n",
    "- Sliding the filter over the width and height of the input volume produces a 2-dimensional activation map.\n",
    "- Given an entire set of filters in each convolutional layer, each of them will produce a separate 2-dimensional activation map. \n",
    "- We will stack these activation maps along the depth dimension and produce the output volume.\n",
    "\n",
    "Each neuron is connected to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the **receptive field** of the neuron (equivalently this is the filter size). The connections are local in space (along width and height), but always full along the entire depth of the input volume.\n",
    "\n",
    "For example, suppose that the input volume has size [32x32x3], (e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 5\\*5\\*3 = 75 weights (and +1 bias parameter).\n",
    "\n",
    "![convolutional-layer](./figures/cnn/convolutional-layer.png)\n",
    "*An example input volume in red (e.g. a 32x32x3 CIFAR-10 image). Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (2 in this example) along the depth, all looking at the same region in the input.*\n",
    "\n",
    "Three hyperparameters control the size of the output volume: the **depth, stride** and **zero-padding**.\n",
    "\n",
    "1. The **depth** of the output volume is a hyperparameter which corresponds to the number of filters, each learning to look for something different in the input. A set of neurons that are all looking at the same region of the input is known as a **depth column** (or *fibre*).\n",
    "2. When the **stride** is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around.\n",
    "3. Sometimes it will be convenient to pad the input volume with zeros around the border. The size of this **zero-padding** is a hyperparameter used to control the spatial size of the output volumes (may use it to exactly preserve the spatial size of the input volume).\n",
    "\n",
    "Given the input volume size (\\\\(W\\times W\\\\)), the receptive field size of the Conv Layer neurons (\\\\(F\\times F\\\\)), the stride (\\\\(S\\\\)), and the amount of zero padding used (\\\\(P\\\\)) on the border, the spatial size of the output volume is $W'\\times W'$ where\n",
    "$$\n",
    "W' = (W - F + 2P)/S + 1\n",
    "$$\n",
    "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output.\n",
    "\n",
    "Sizing the ConvNets appropriately so that all the dimensions \"work out\" can be a real headache, which the use of zero-padding and some design guidelines will significantly alleviate.\n",
    "\n",
    "**Parameter Sharing** scheme is used in Convolutional Layers to control the number of parameters by making one reasonable assumption: if one feature is useful to compute at some spatial position (x,y), then it should also be useful to compute at a different position (x2,y2). In other words, denoting a single 2-dimensional slice of depth as a **depth slice**, we are going to constrain the neurons in each depth slice to use the **same weights and bias**. In practice during backpropagation, every neuron in the volume will compute the gradient for its weights, but these gradients will be added up across each depth slice and only update a single set of weights per slice.\n",
    "\n",
    "Notice that if all neurons in a single depth slice are using the same weight vector, then the forward pass of the CONV layer can in each depth slice be computed as a **convolution** of the neuron's weights with the input volume (Hence the name: Convolutional Layer). This is why it is common to refer to the sets of weights as a **filter** (or a **kernel**), that is convolved with the input.\n",
    "![example-filters](./figures/cnn/weights.jpeg)\n",
    "*Example filters learned by [Krizhevsky et al.](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks). Each of the 96 filters shown here is of size [11x11x3], and each one is shared by the 55*55 neurons in one depth slice. Notice that the parameter sharing assumption is relatively reasonable: If detecting a horizontal edge is important at some location in the image, it should intuitively be useful at some other location as well due to the translationally-invariant structure of images. There is therefore no need to relearn to detect a horizontal edge at every one of the 55*55 distinct locations in the Conv layer output volume.*\n",
    "\n",
    "Note that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a ConvNet have some specific centered structure, where we should expect, for example, that completely different features should be learned on one side of the image than another. One practical example is when the input are faces that have been centered in the image. You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a **Locally-Connected Layer**.\n",
    "\n",
    "**Summary**. To summarize, the Conv Layer:\n",
    "- Accepts a volume of size \\\\(W_1 \\times H_1 \\times D_1\\\\)\n",
    "- Requires four hyperparameters: number of filters \\\\(K\\\\), their spatial extent \\\\(F\\\\), the stride \\\\(S\\\\) and the amount of zero padding \\\\(P\\\\).\n",
    "- Produces a volume of size \\\\(W_2 \\times H_2 \\times D_2\\\\) where:\n",
    "  - \\\\(W_2 = (W_1 - F + 2P)/S + 1\\\\)\n",
    "  - \\\\(H_2 = (H_1 - F + 2P)/S + 1\\\\)\n",
    "  - \\\\(D_2 = K\\\\)\n",
    "- With parameter sharing, it introduces \\\\(F \\cdot F \\cdot D_1\\\\) weights per filter, for a total of \\\\((F \\cdot F \\cdot D_1) \\cdot K\\\\) weights and \\\\(K\\\\) biases.\n",
    "- In the output volume, the \\\\(d\\\\)-th depth slice (of size \\\\(W_2 \\times H_2\\\\)) is the result of performing a valid convolution of the \\\\(d\\\\)-th filter over the input volume with a stride of \\\\(S\\\\), and then offset by \\\\(d\\\\)-th bias.\n",
    "\n",
    "A common setting of the hyperparameters is \\\\(F = 3, S = 1, P = 1\\\\). However, there are common conventions and rules of thumb that motivate these hyperparameters. See the [ConvNet architectures](#architectures) section below.\n",
    "\n",
    "**Implementation as Matrix Multiplication**.\n",
    "\n",
    "1. The local regions in the input image are stretched out into columns in an operation commonly called **im2col**. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take [11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11\\*11\\*3 = 363. Iterating this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an output matrix `X_col` of *im2col* of size [363 x 3025], where every column is a stretched out receptive field and there are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may be duplicated in multiple distinct columns.\n",
    "2. The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size [11x11x3] this would give a matrix `W_row` of size [96 x 363].\n",
    "3. The result of a convolution is now equivalent to performing one large matrix multiply `np.dot(W_row, X_col)`, which evaluates the dot product between every filter and every receptive field location. In our example, the output of this operation would be [96 x 3025], giving the output of the dot product of each filter at each location. \n",
    "4. The result must finally be reshaped back to its proper output dimension [55x55x96].\n",
    "\n",
    "Some values in the input volume are replicated multiple times in `X_col` using a lot of memory, however the benefit is that there are many very efficient implementations of Matrix Multiplication that we can take advantage of (for example, in the commonly used [BLAS](http://www.netlib.org/blas/) API). Moreover, the same *im2col* idea can be reused to perform the pooling operation.\n",
    "\n",
    "**Backpropagation.** The backward pass for a convolution operation (for both the data and the weights) is also a convolution (but with spatially-flipped filters).\n",
    "\n",
    "**1x1 convolution**. As an aside, several papers use 1x1 convolutions, as first investigated by [Network in Network](http://arxiv.org/abs/1312.4400). For example, if the input is [32x32x3] then doing 1x1 convolutions would effectively be doing 3-dimensional dot products (since the input depth is 3 channels).\n",
    "\n",
    "**Dilated convolutions.** It is possible to have filters that have spaces between each cell, called dilation ([paper by Fisher Yu and Vladlen Koltun](https://arxiv.org/abs/1511.07122)). It allows you to merge spatial information across the inputs much more agressively with fewer layers. For example, if you stack two 3x3 CONV layers on top of each other then the neurons on the 2nd layer are a function of a 5x5 patch of the input (we would say that the *effective receptive field* of these neurons is 5x5). If we use dilated convolutions then this effective receptive field would grow much quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='pool'></a>\n",
    "\n",
    "### Pooling Layer\n",
    "\n",
    "It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. More generally, the pooling layer:\n",
    "\n",
    "- Accepts a volume of size \\\\(W_1 \\times H_1 \\times D_1\\\\)\n",
    "- Requires two hyperparameters: their spatial extent \\\\(F\\\\) and the stride \\\\(S\\\\), \n",
    "- Produces a volume of size \\\\(W_2 \\times H_2 \\times D_2\\\\) where:\n",
    "  - \\\\(W_2 = (W_1 - F)/S + 1\\\\)\n",
    "  - \\\\(H_2 = (H_1 - F)/S + 1\\\\)\n",
    "  - \\\\(D_2 = D_1\\\\)\n",
    "- Introduces zero parameters since it computes a fixed function of the input\n",
    "- Note that it is not common to use zero-padding for Pooling layers\n",
    "\n",
    "There are only two commonly seen variations of the max pooling layer found in practice: A pooling layer with \\\\(F = 3, S = 2\\\\) (also called overlapping pooling), and more commonly \\\\(F = 2, S = 2\\\\). Pooling sizes with larger receptive fields are too destructive.\n",
    "\n",
    "**General pooling**. In addition to max pooling, the pooling units can also perform other functions, such as *average pooling* or even *L2-norm pooling*. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.\n",
    "![max-pooling](./figures/cnn/max-pooling.png)\n",
    "*Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. <b>Left:</b> In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. <b>Right:</b> The most common downsampling operation is max, giving rise to <b>max pooling</b>, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).*\n",
    "\n",
    "**Backpropagation**. The backward pass for a max(x, y) operation has a simple interpretation as only routing the gradient to the input that had the highest value in the forward pass. Hence, during the forward pass of a pooling layer it is common to keep track of the index of the max activation (sometimes also called *the switches*) so that gradient routing is efficient during backpropagation.\n",
    "\n",
    "**Getting rid of pooling**. Many people think that we can get away without it. For example, [Striving for Simplicity: The All Convolutional Net](http://arxiv.org/abs/1412.6806) proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size of the representation they suggest using larger stride in CONV layer once in a while. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name='norm'></a>\n",
    "\n",
    "### Normalization Layer\n",
    "Normalization layers have since fallen out of favor because in practice their contribution has been shown to be minimal, if any. For various types of normalizations, see the discussion in Alex Krizhevsky's [cuda-convnet library API](http://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='convert'></a>\n",
    "\n",
    "### Converting FC layers to CONV layers \n",
    "\n",
    "It's possible to convert between FC and CONV layers:\n",
    "\n",
    "- For any CONV layer there is an FC layer that implements the same forward function. The weight matrix would be a large matrix that is mostly zero except for at certain blocks (due to local connectivity) where the weights in many of the blocks are equal (due to parameter sharing).\n",
    "- Conversely, any FC layer can be converted to a CONV layer by setting the filter size to be exactly the size of the input volume.\n",
    "\n",
    "The ability to convert an FC layer to a CONV layer is particularly useful in practice. It turns out that this conversion allows us to \"slide\" the original ConvNet very efficiently across many spatial positions in a larger image, in a single forward pass. \n",
    "\n",
    "- An IPython Notebook on [Net Surgery](https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb) shows how to perform the conversion in practice, in code (using Caffe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='architectures'></a>\n",
    "## ConvNet Architectures\n",
    "\n",
    "<a name='layerpat'></a>\n",
    "\n",
    "### Layer Patterns\n",
    "The most common ConvNet architecture follows the pattern:\n",
    "\n",
    "`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`\n",
    "\n",
    "where the `*` indicates repetition, the `POOL?` indicates an optional pooling layer,\n",
    "`N >= 0` (and usually `N <= 3`), `M >= 0`, `K >= 0` (and usually `K < 3`).\n",
    "\n",
    "Prefer a stack of small filter CONV layers to one large receptive field CONV layer.\n",
    "\n",
    "- The stacks of CONV layers contain non-linearities that make their features more expressive.\n",
    "- Stacking CONV layers require fewer parameters.\n",
    "\n",
    "As a practical disadvantage, we might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation.\n",
    "\n",
    "**Recent departures.** The conventional paradigm of a linear list of layers has recently been challenged, in Google's Inception architectures and also in current (state of the art) Residual Networks from Microsoft Research Asia. Both of these (see details below in case studies section) feature more intricate and different connectivity structures.\n",
    "\n",
    "**In practice: use whatever works best on ImageNet**. Rarely ever have to train a ConvNet from scratch or design one from scratch.\n",
    "\n",
    "<a name='layersizepat'></a>\n",
    "\n",
    "### Layer Sizing Patterns\n",
    "\n",
    "- The **input layer** (that contains the image) should be divisible by 2 many times.\n",
    "\n",
    "- The **conv layers** should be using small filters (e.g. 3x3 or at most 5x5), using a stride of \\\\(S = 1\\\\), and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. \n",
    "\n",
    "- The **pool layers** are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. \\\\(F = 2\\\\)), and with a stride of 2 (i.e. \\\\(S = 2\\\\)). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both width and height). \n",
    "\n",
    "The scheme presented above is pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling the volumes spatially.\n",
    "\n",
    "*Why use stride of 1 in CONV?* \n",
    "- Smaller strides work better in practice.\n",
    "- Stride 1 allows us to leave all spatial down-sampling to the POOL layers.\n",
    "\n",
    "*Why use padding?*\n",
    "- Keep the spatial sizes constant after CONV.\n",
    "- Improves performance.\n",
    "\n",
    "*Compromising based on memory constraints.* Since GPUs are often bottlenecked by memory, it may be necessary to compromise. In practice, people prefer to make the compromise at only the first CONV layer of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='case'></a>\n",
    "\n",
    "### Case studies\n",
    "\n",
    "There are several architectures in the field of Convolutional Networks that have a name. The most common are:\n",
    "\n",
    "- **[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)** is the first successful applications of Convolutional Networks were developed by Yann LeCun in 1990's.\n",
    "![LeNet](./figures/cnn/LeNet-5.png)\n",
    "- **[AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)** is the first work that popularized Convolutional Networks in Computer Vision developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was submitted to the [ImageNet ILSVRC challenge](http://www.image-net.org/challenges/LSVRC/2014/) in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error).\n",
    "![AlexNet](./figures/cnn/AlexNet.png)\n",
    "- **[ZFNet](http://arxiv.org/abs/1311.2901)** is the ILSVRC 2013 winner developed by Matthew Zeiler and Rob Fergus. It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller.\n",
    "![ZFNet](./figures/cnn/ZFNet.png)\n",
    "- **[GoogLeNet](http://arxiv.org/abs/1409.4842)** is the ILSVRC 2014 winner developed by Szegedy et al. from Google. Its main contribution was the development of an *Inception Module* that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. There are also several followup versions to the GoogLeNet, most recently [Inception-v4](http://arxiv.org/abs/1602.07261).\n",
    "- **[VGGNet](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)** is the runner-up in ILSVRC 2014 developed by Karen Simonyan and Andrew Zisserman. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their [pretrained model](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) is available for plug and play use in Caffe. It was since found that FC layers in VGGNet can be removed with no performance downgrade, significantly reducing the number of necessary parameters.\n",
    "- **[ResNet](http://arxiv.org/abs/1512.03385)** was developed by Kaiming He et al, the winner of ILSVRC 2015. It features special *skip connections* and a heavy use of [batch normalization](http://arxiv.org/abs/1502.03167). The architecture is also missing fully connected layers at the end of the network. The reader is also referred to Kaiming's presentation ([video](https://www.youtube.com/watch?v=1PGLj-uKT1w), [slides](http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)), and some [recent experiments](https://github.com/gcr/torch-residual-networks) that reproduce these networks in Torch. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 10, 2016). In particular, also see more recent developments that tweak the original architecture from [Kaiming He et al. Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (published March 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VGGNet in detail**.\n",
    "The whole VGGNet is composed of CONV layers that perform 3x3 convolutions with stride 1 and pad 1, and of POOL layers that perform 2x2 max pooling with stride 2 (and no padding).\n",
    "\n",
    "```\n",
    "INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\n",
    "CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\n",
    "CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\n",
    "POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\n",
    "CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\n",
    "CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\n",
    "POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\n",
    "CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\n",
    "CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\n",
    "CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\n",
    "POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\n",
    "CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\n",
    "CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\n",
    "CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\n",
    "POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\n",
    "CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\n",
    "CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\n",
    "CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\n",
    "POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\n",
    "FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\n",
    "FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\n",
    "FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n",
    "\n",
    "TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)\n",
    "TOTAL params: 138M parameters\n",
    "```\n",
    "\n",
    "As is common with Convolutional Networks, notice that most of the memory (and also compute time) is used in the early CONV layers, and that most of the parameters are in the last FC layers. In this particular case, the first FC layer contains 100M weights, out of a total of 140M.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='comp'></a>\n",
    "\n",
    "### Computational Considerations\n",
    "\n",
    "The largest bottleneck to be aware of when constructing ConvNet architectures is the memory bottleneck. There are three major sources of memory to keep track of:\n",
    "\n",
    "- The intermediate volume sizes. These are the raw number of **activations** at every layer of the ConvNet, and also their gradients (of equal size). A clever implementation that runs a ConvNet only at test time could in principle reduce this by a huge amount, by only storing the current activations at any layer and discarding the previous activations on layers below.\n",
    "- The parameter sizes. These are the numbers that hold the network **parameters**, their gradients during backpropagation, and commonly also a step cache if the optimization is using momentum, Adagrad, or RMSProp. Therefore, the memory to store the parameter vector alone must usually be multiplied by a factor of at least 3 or so.\n",
    "- Every ConvNet implementation has to maintain **miscellaneous** memory, such as the image data batches, perhaps their augmented versions, etc.\n",
    "\n",
    "If your network doesn't fit, a common heuristic to \"make it fit\" is to decrease the batch size, since most of the memory is usually consumed by the activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='add'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "Additional resources related to implementation:\n",
    "\n",
    "- [Soumith benchmarks for CONV performance](https://github.com/soumith/convnet-benchmarks)\n",
    "- [ConvNetJS CIFAR-10 demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html) allow to play with ConvNet architectures in real time, in the browser.\n",
    "- [Caffe](http://caffe.berkeleyvision.org/), one of the popular ConvNet libraries.\n",
    "- [State of the art ResNets in Torch7](http://torch.ch/blog/2016/02/04/resnets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
