{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "Table of Contents:\n",
    "\n",
    "- [Architecture Overview](#overview)\n",
    "- [ConvNet Layers](#layers)\n",
    "  - [Convolutional Layer](#conv)\n",
    "  - [Pooling Layer](#pool)\n",
    "  - [Normalization Layer](#norm)\n",
    "  - [Fully-Connected Layer](#fc)\n",
    "  - [Converting Fully-Connected Layers to Convolutional Layers](#convert)\n",
    "- [ConvNet Architectures](#architectures)\n",
    "  - [Layer Patterns](#layerpat)\n",
    "  - [Layer Sizing Patterns](#layersizepat)\n",
    "  - [Case Studies](#case) (LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet)\n",
    "  - [Computational Considerations](#comp)\n",
    "- [Additional References](#add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='overview'></a>\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Regular Neural Nets don't scale well to full images. **Convolutional Neural Networks** take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular: \n",
    "- The layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. \n",
    "- The neurons in a layer will only be connected to a small region of the layer before it. \n",
    "- By the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension. \n",
    "\n",
    "Here is a visualization:\n",
    "\n",
    "![convolutional-neural-networks](./figures/cnn/cnn.jpeg)\n",
    "\n",
    "*A ConvNet arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels).*\n",
    "\n",
    "> A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='layers'></a>\n",
    "\n",
    "## Layers used to build ConvNets \n",
    "\n",
    "There are three main types of layers to build ConvNet architectures: **Convolutional Layer**, **Pooling Layer**, and **Fully-Connected Layer**.\n",
    "\n",
    "A simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC].\n",
    "\n",
    "- **INPUT** [32x32x3] will hold the raw pixel values of the image. An image of width 32, height 32, and with three color channels R,G,B.\n",
    "- **CONV layer** will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.\n",
    "- **RELU layer** will apply an elementwise activation function, such as the \\\\(max(0,x)\\\\) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).\n",
    "- **POOL layer** will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].\n",
    "- **FC (i.e. fully-connected) layer** will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score. Each neuron in this layer will be connected to all the numbers in the previous volume.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)\n",
    "- There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)\n",
    "- Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function\n",
    "- Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don't)\n",
    "- Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn't)\n",
    "\n",
    "![convnet-car](./figures/cnn/convnet-car.jpeg)\n",
    "*The activations of an example ConvNet architecture. The initial volume stores the raw image pixels (left) and the last volume stores the class scores (right). Each volume of activations along the processing path is shown as a column. Since it's difficult to visualize 3D volumes, we lay out each volume's slices in rows. The last layer volume holds the scores for each class, but here we only visualize the sorted top 5 scores, and print the labels of each one. The architecture shown here is a tiny VGG Net.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "\n",
    "**Overview and intuition.** \n",
    "- The convolutional layer's parameters consist of a set of learnable filters. \n",
    "- Every filter is small spatially (along width and height), but extends through the full depth of the input volume. \n",
    "- Sliding the filter over the width and height of the input volume produces a 2-dimensional activation map.\n",
    "- Given an entire set of filters in each convolutional layer, each of them will produce a separate 2-dimensional activation map. \n",
    "- We will stack these activation maps along the depth dimension and produce the output volume.\n",
    "\n",
    "Each neuron is connected to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the **receptive field** of the neuron (equivalently this is the filter size). The connections are local in space (along width and height), but always full along the entire depth of the input volume.\n",
    "\n",
    "For example, suppose that the input volume has size [32x32x3], (e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 5\\*5\\*3 = 75 weights (and +1 bias parameter).\n",
    "\n",
    "![convolutional-layer](./figures/cnn/convolutional-layer.png)\n",
    "*An example input volume in red (e.g. a 32x32x3 CIFAR-10 image). Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (2 in this example) along the depth, all looking at the same region in the input.*\n",
    "\n",
    "Three hyperparameters control the size of the output volume: the **depth, stride** and **zero-padding**.\n",
    "\n",
    "1. The **depth** of the output volume is a hyperparameter which corresponds to the number of filters, each learning to look for something different in the input. A set of neurons that are all looking at the same region of the input is known as a **depth column** (or *fibre*).\n",
    "2. When the **stride** is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around.\n",
    "3. Sometimes it will be convenient to pad the input volume with zeros around the border. The size of this **zero-padding** is a hyperparameter used to control the spatial size of the output volumes (may use it to exactly preserve the spatial size of the input volume).\n",
    "\n",
    "Given the input volume size (\\\\(W\\times W\\\\)), the receptive field size of the Conv Layer neurons (\\\\(F\\times F\\\\)), the stride (\\\\(S\\\\)), and the amount of zero padding used (\\\\(P\\\\)) on the border, the spatial size of the output volume is $W'\\times W'$ where\n",
    "$$\n",
    "W' = (W - F + 2P)/S + 1\n",
    "$$\n",
    "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output.\n",
    "\n",
    "Sizing the ConvNets appropriately so that all the dimensions \"work out\" can be a real headache, which the use of zero-padding and some design guidelines will significantly alleviate.\n",
    "\n",
    "**Parameter Sharing** scheme is used in Convolutional Layers to control the number of parameters by making one reasonable assumption: if one feature is useful to compute at some spatial position (x,y), then it should also be useful to compute at a different position (x2,y2). In other words, denoting a single 2-dimensional slice of depth as a **depth slice**, we are going to constrain the neurons in each depth slice to use the **same weights and bias**. In practice during backpropagation, every neuron in the volume will compute the gradient for its weights, but these gradients will be added up across each depth slice and only update a single set of weights per slice.\n",
    "\n",
    "Notice that if all neurons in a single depth slice are using the same weight vector, then the forward pass of the CONV layer can in each depth slice be computed as a **convolution** of the neuron's weights with the input volume (Hence the name: Convolutional Layer). This is why it is common to refer to the sets of weights as a **filter** (or a **kernel**), that is convolved with the input.\n",
    "![example-filters](./figures/cnn/weights.jpeg)\n",
    "*Example filters learned by [Krizhevsky et al.](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks). Each of the 96 filters shown here is of size [11x11x3], and each one is shared by the 55*55 neurons in one depth slice. Notice that the parameter sharing assumption is relatively reasonable: If detecting a horizontal edge is important at some location in the image, it should intuitively be useful at some other location as well due to the translationally-invariant structure of images. There is therefore no need to relearn to detect a horizontal edge at every one of the 55*55 distinct locations in the Conv layer output volume.*\n",
    "\n",
    "Note that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a ConvNet have some specific centered structure, where we should expect, for example, that completely different features should be learned on one side of the image than another. One practical example is when the input are faces that have been centered in the image. You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a **Locally-Connected Layer**.\n",
    "\n",
    "**Summary**. To summarize, the Conv Layer:\n",
    "- Accepts a volume of size \\\\(W_1 \\times H_1 \\times D_1\\\\)\n",
    "- Requires four hyperparameters: number of filters \\\\(K\\\\), their spatial extent \\\\(F\\\\), the stride \\\\(S\\\\) and the amount of zero padding \\\\(P\\\\).\n",
    "- Produces a volume of size \\\\(W_2 \\times H_2 \\times D_2\\\\) where:\n",
    "  - \\\\(W_2 = (W_1 - F + 2P)/S + 1\\\\)\n",
    "  - \\\\(H_2 = (H_1 - F + 2P)/S + 1\\\\)\n",
    "  - \\\\(D_2 = K\\\\)\n",
    "- With parameter sharing, it introduces \\\\(F \\cdot F \\cdot D_1\\\\) weights per filter, for a total of \\\\((F \\cdot F \\cdot D_1) \\cdot K\\\\) weights and \\\\(K\\\\) biases.\n",
    "- In the output volume, the \\\\(d\\\\)-th depth slice (of size \\\\(W_2 \\times H_2\\\\)) is the result of performing a valid convolution of the \\\\(d\\\\)-th filter over the input volume with a stride of \\\\(S\\\\), and then offset by \\\\(d\\\\)-th bias.\n",
    "\n",
    "A common setting of the hyperparameters is \\\\(F = 3, S = 1, P = 1\\\\). However, there are common conventions and rules of thumb that motivate these hyperparameters. See the [ConvNet architectures](#architectures) section below.\n",
    "\n",
    "**Implementation as Matrix Multiplication**.\n",
    "\n",
    "1. The local regions in the input image are stretched out into columns in an operation commonly called **im2col**. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take [11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11\\*11\\*3 = 363. Iterating this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an output matrix `X_col` of *im2col* of size [363 x 3025], where every column is a stretched out receptive field and there are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may be duplicated in multiple distinct columns.\n",
    "2. The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size [11x11x3] this would give a matrix `W_row` of size [96 x 363].\n",
    "3. The result of a convolution is now equivalent to performing one large matrix multiply `np.dot(W_row, X_col)`, which evaluates the dot product between every filter and every receptive field location. In our example, the output of this operation would be [96 x 3025], giving the output of the dot product of each filter at each location. \n",
    "4. The result must finally be reshaped back to its proper output dimension [55x55x96].\n",
    "\n",
    "Some values in the input volume are replicated multiple times in `X_col` using a lot of memory, however the benefit is that there are many very efficient implementations of Matrix Multiplication that we can take advantage of (for example, in the commonly used [BLAS](http://www.netlib.org/blas/) API). Moreover, the same *im2col* idea can be reused to perform the pooling operation.\n",
    "\n",
    "**Backpropagation.** The backward pass for a convolution operation (for both the data and the weights) is also a convolution (but with spatially-flipped filters).\n",
    "\n",
    "**1x1 convolution**. As an aside, several papers use 1x1 convolutions, as first investigated by [Network in Network](http://arxiv.org/abs/1312.4400). For example, if the input is [32x32x3] then doing 1x1 convolutions would effectively be doing 3-dimensional dot products (since the input depth is 3 channels).\n",
    "\n",
    "**Dilated convolutions.** It is possible to have filters that have spaces between each cell, called dilation ([paper by Fisher Yu and Vladlen Koltun](https://arxiv.org/abs/1511.07122)). It allows you to merge spatial information across the inputs much more agressively with fewer layers. For example, if you stack two 3x3 CONV layers on top of each other then the neurons on the 2nd layer are a function of a 5x5 patch of the input (we would say that the *effective receptive field* of these neurons is 5x5). If we use dilated convolutions then this effective receptive field would grow much quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='pool'></a>\n",
    "\n",
    "### Pooling Layer\n",
    "\n",
    "It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. More generally, the pooling layer:\n",
    "\n",
    "- Accepts a volume of size \\\\(W_1 \\times H_1 \\times D_1\\\\)\n",
    "- Requires two hyperparameters: their spatial extent \\\\(F\\\\) and the stride \\\\(S\\\\), \n",
    "- Produces a volume of size \\\\(W_2 \\times H_2 \\times D_2\\\\) where:\n",
    "  - \\\\(W_2 = (W_1 - F)/S + 1\\\\)\n",
    "  - \\\\(H_2 = (H_1 - F)/S + 1\\\\)\n",
    "  - \\\\(D_2 = D_1\\\\)\n",
    "- Introduces zero parameters since it computes a fixed function of the input\n",
    "- Note that it is not common to use zero-padding for Pooling layers\n",
    "\n",
    "There are only two commonly seen variations of the max pooling layer found in practice: A pooling layer with \\\\(F = 3, S = 2\\\\) (also called overlapping pooling), and more commonly \\\\(F = 2, S = 2\\\\). Pooling sizes with larger receptive fields are too destructive.\n",
    "\n",
    "**General pooling**. In addition to max pooling, the pooling units can also perform other functions, such as *average pooling* or even *L2-norm pooling*. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.\n",
    "![max-pooling](./figures/cnn/max-pooling.png)\n",
    "*Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. <b>Left:</b> In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. <b>Right:</b> The most common downsampling operation is max, giving rise to <b>max pooling</b>, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).*\n",
    "\n",
    "**Backpropagation**. Recall from the backpropagation chapter that the backward pass for a max(x, y) operation has a simple interpretation as only routing the gradient to the input that had the highest value in the forward pass. Hence, during the forward pass of a pooling layer it is common to keep track of the index of the max activation (sometimes also called *the switches*) so that gradient routing is efficient during backpropagation.\n",
    "\n",
    "**Getting rid of pooling**. Many people dislike the pooling operation and think that we can get away without it. For example, [Striving for Simplicity: The All Convolutional Net](http://arxiv.org/abs/1412.6806) proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size of the representation they suggest using larger stride in CONV layer once in a while. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
